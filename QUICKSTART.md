# ğŸ¯ å¿«é€Ÿå¼€å§‹æŒ‡å—

## 5 åˆ†é’Ÿå¿«é€Ÿéƒ¨ç½²æœ¬åœ°è¯­éŸ³å¯¹è¯ç³»ç»Ÿ

### ğŸ“‹ å‰ç½®æ¡ä»¶

âœ… Docker Desktop å·²å®‰è£…
âœ… Ollama å·²å®‰è£…å¹¶è¿è¡Œ (ç”¨äºæœ¬åœ° LLM)
   - Mac/Linux: `curl -fsSL https://ollama.com/install.sh | sh`
   - Windows: https://ollama.com/download

---

## ğŸš€ å››æ­¥å¯åŠ¨

### 0ï¸âƒ£ å¯åŠ¨ Ollama å¹¶ä¸‹è½½æ¨¡å‹

```bash
# å¯åŠ¨ Ollama æœåŠ¡
ollama serve

# æ–°ç»ˆç«¯: ä¸‹è½½æ¨èæ¨¡å‹ (çº¦ 400MB)
ollama pull qwen2.5:0.5b
```

### 1ï¸âƒ£ é…ç½®ç¯å¢ƒå˜é‡

```bash
# å¤åˆ¶é…ç½®æ¨¡æ¿
cp .env.example .env

# ç¼–è¾‘ .env
# Ollama é…ç½® (é»˜è®¤å·²é…ç½®å¥½)
USE_LOCAL_LLM=1
OLLAMA_BASE_URL=http://host.docker.internal:11434/v1
OLLAMA_MODEL=qwen2.5:0.5b

# å¦‚æœéœ€è¦è¿œç¨‹ APIï¼Œè®¾ç½®:
# USE_LOCAL_LLM=0
# VITE_OPENAI_API_KEY=sk-your-key-here
# LLM_API_KEY=sk-your-key-here
```

### 2ï¸âƒ£ ä¸‹è½½ TTS/ASR æ¨¡å‹

```bash
# è¿è¡Œæ¨¡å‹ä¸‹è½½è„šæœ¬ (çº¦ 600MB)
./scripts/download_models.sh
```

### 3ï¸âƒ£ å¯åŠ¨æœåŠ¡

```bash
# ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ
# æ–°ç»ˆç«¯æ£€æŸ¥: curl http://localhost:11434/api/tags

# ä¸€é”®å¯åŠ¨ (Docker)
./scripts/start.sh

# æˆ–æ‰‹åŠ¨å¯åŠ¨
docker compose up --build
```

**è®¿é—®**: http://localhost:5173

---
1. æ‰“å¼€æµè§ˆå™¨è®¿é—® http://localhost:5173
2. é€‰æ‹© "ğŸ  Local Model (Fun-ASR + CosyVoice)"
3. ç‚¹å‡» "Connect" æŒ‰é’®
4. å…è®¸éº¦å…‹é£æƒé™
5. å¼€å§‹è¯´è¯ï¼Œç³»ç»Ÿä¼šï¼š
   - å®æ—¶è¯†åˆ«ä½ çš„è¯­éŸ³ (Fun-ASR)
   - è°ƒç”¨æœ¬åœ° LLM ç”Ÿæˆå›å¤ (Ollama Qwen 2.5)
   - æµå¼åˆæˆè¯­éŸ³æ’­æ”¾ (CosyVoice)

**âœ… å®Œå…¨æœ¬åœ°åŒ–ï¼Œæ— éœ€ä»»ä½•è¿œç¨‹ APIï¼** | ä¸‹è½½å‘½ä»¤ |
|------|------|------|------|----------|
| **qwen2.5:0.5b** â­ | 400MB | æå¿« | ä¼˜ç§€ | `ollama pull qwen2.5:0.5b` |
| qwen2.5:1.5b | 1GB | å¿« | æ›´å¥½ | `ollama pull qwen2.5:1.5b` |
| qwen2.5:3b | 2GB | ä¸­ç­‰ | æœ€å¥½ | `ollama pull qwen2.5:3b` |

### åˆ‡æ¢æ¨¡å‹

ç¼–è¾‘ `.env`:
```bash
OLLAMA_MODEL=qwen2.5:1.5b  # ä½¿ç”¨ 1.5B æ¨¡å‹
```

**è¯¦ç»†é…ç½®**: æŸ¥çœ‹ [OLLAMA_SETUP.md](OLLAMA_SETUP.md)

---

## ğŸ›ï¸ ä½¿ç”¨æ–¹æ³•

1. æ‰“å¼€æµè§ˆå™¨è®¿é—® http://localhost:5173
2. é€‰æ‹© "ğŸ  Local Model (Fun-ASR + CosyVoice)"
3. ç‚¹å‡» "Connect" æŒ‰é’®
4. å…è®¸éº¦å…‹é£æƒé™
5. å¼€å§‹è¯´è¯ï¼Œç³»ç»Ÿä¼šï¼š
   - å®æ—¶è¯†åˆ«ä½ çš„è¯­éŸ³ (ASR)
   - è°ƒç”¨ LLM ç”Ÿæˆå›å¤
   - æµå¼åˆæˆè¯­éŸ³æ’­æ”¾ (TTS)

---

## ğŸ³ Docker Image è¯´æ˜

### æ¨èé…ç½®

**æœ‰ NVIDIA GPU (æ¨è):**
```yaml
# docker-compose.yml å·²é…ç½®
# ä½¿ç”¨ nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04
```
- âœ… GPU åŠ é€Ÿï¼Œå®æ—¶å“åº”
- âš ï¸ éœ€è¦å®‰è£… NVIDIA Docker Runtime

**ä»… CPU (å¤‡é€‰):**
```yaml
# ä¿®æ”¹ docker-compose.yml
# 1. æ³¨é‡Šæ‰ deploy.resources éƒ¨åˆ†
# 2. ä¿®æ”¹ Dockerfile:
#    FROM python:3.10-slim
# 3. æ·»åŠ ç¯å¢ƒå˜é‡ USE_CPU=1
```
- âœ… æ— éœ€ GPU
- âš ï¸ æ¨ç†é€Ÿåº¦æ…¢ (å»¶è¿Ÿ 2-3 ç§’)

**Apple Silicon Mac:**
```bash
# Docker ä¸æ”¯æŒ GPU é€ä¼ 
# å»ºè®®ç›´æ¥æœ¬åœ° Python è¿è¡Œ
python3 -m venv venv
source venv/bin/activate
pip install -r backend/requirements.txt
python backend/server.py
```
- âœ… è‡ªåŠ¨ä½¿ç”¨ MPS åŠ é€Ÿ

---

## ğŸ“¦ é¡¹ç›®å¦‚ä½•åœ¨ Docker ä¸­è¿è¡Œ

### æ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Docker Container: frontend        â”‚
â”‚   Image: node:20-alpine             â”‚
â”‚   - è¿è¡Œ Vite å¼€å‘æœåŠ¡å™¨ (5173)    â”‚
â”‚   - è¿è¡Œ Express token æœåŠ¡ (3000) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ é€šè¿‡ Docker ç½‘ç»œè¿æ¥
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Docker Container: backend         â”‚
â”‚   Image: nvidia/cuda:11.8 æˆ–        â”‚
â”‚          python:3.10-slim           â”‚
â”‚   - FastAPI WebSocket æœåŠ¡ (8000)  â”‚
â”‚   - Fun-ASR è¯­éŸ³è¯†åˆ«               â”‚
â”‚   - CosyVoice TTS åˆæˆ             â”‚
â”‚   - OpenAI LLM æ¥å£                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ•°æ®æµ

```
æµè§ˆå™¨ â†’ http://localhost:5173 (å‰ç«¯)
   â†“
   WebSocket â†’ ws://localhost:8000/ws (åç«¯)
   â†“
   éº¦å…‹é£éŸ³é¢‘ â†’ Fun-ASR â†’ æ–‡æœ¬
   â†“
   æ–‡æœ¬ â†’ OpenAI LLM â†’ å›å¤æ–‡æœ¬
   â†“
   å›å¤æ–‡æœ¬ â†’ CosyVoice â†’ éŸ³é¢‘æµ
   â†“
   éŸ³é¢‘æµ â†’ æµè§ˆå™¨æ’­æ”¾
```

### æ–‡ä»¶æŒ‚è½½

```yaml
volumes:
  - ./backend:/app              # åç«¯ä»£ç çƒ­é‡è½½
  - model-cache:/root/.cache    # æ¨¡å‹æ–‡ä»¶æŒä¹…åŒ–
  - .:/app                      # å‰ç«¯ä»£ç 
  - /app/node_modules           # å®¹å™¨å†… node_modules
```

---

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: GPU æ€ä¹ˆåœ¨ Docker ä¸­ä½¿ç”¨ï¼Ÿ

**A:** ä½¿ç”¨ NVIDIA Docker Runtime

```bash
# Linux å®‰è£…
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker

# éªŒè¯
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# docker-compose.yml é…ç½®
services:
  backend:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
### Q2: Mac èƒ½ç”¨ GPU åŠ é€Ÿå—ï¼Ÿ

**A:** Docker Desktop ä¸æ”¯æŒ GPU é€ä¼ 

- âŒ Docker ä¸­æ— æ³•ä½¿ç”¨ GPU
- âœ… ç›´æ¥æœ¬åœ° Python è¿è¡Œå¯ä½¿ç”¨ MPS
- âœ… PyTorch è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨ MPS
- âœ… Ollama è‡ªåŠ¨ä½¿ç”¨ Metal åŠ é€Ÿ

```python
# è‡ªåŠ¨ä½¿ç”¨ MPS (Metal Performance Shaders)
device = "mps" if torch.backends.mps.is_available() else "cpu"
```

### Q3: Ollama è¿æ¥å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A:** æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€

```bash
# æ£€æŸ¥æœåŠ¡
curl http://localhost:11434/api/tags

**ä¼˜åŒ–å»ºè®®**:
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ (CosyVoice-300M)
- Ollama ä½¿ç”¨ qwen2.5:0.5b (æœ€å¿«)
- é™ä½éŸ³é¢‘é‡‡æ ·ç‡
- è€ƒè™‘äº‘ GPU æœåŠ¡ (AWS/GCP)

### Q5: æ¨¡å‹æ–‡ä»¶å¤ªå¤§æ€ä¹ˆåŠï¼Ÿ
# åº”è¯¥çœ‹åˆ°: âœ… Local LLM initialized: qwen2.5:0.5b
```

**Docker è¿æ¥é…ç½®**:
```yaml
volumes:
  - model-cache:/root/.cache  # æŒä¹…åŒ–ç¼“å­˜
```

- CosyVoice + FunASR: ~10GB
- Ollama qwen2.5:0.5b: ~400MB
- åç»­å¯åŠ¨: ç›´æ¥ä½¿ç”¨ç¼“å­˜
- æ¸…ç†ç¼“å­˜: `docker compose down -v`

**Ollama æ¨¡å‹ç®¡ç†**:
```bash
# æŸ¥çœ‹å·²ä¸‹è½½æ¨¡å‹
ollama list

# åˆ é™¤ä¸ç”¨çš„æ¨¡å‹
ollama rm model-name
```

---

## ğŸ“š å®Œæ•´æ–‡æ¡£

- **Ollama é…ç½®**: [OLLAMA_SETUP.md](OLLAMA_SETUP.md) â­ æ–°å¢
- **éƒ¨ç½²æŒ‡å—**: [DEPLOYMENT.md](DEPLOYMENT.md)
docker stats
```

---

## ğŸ›‘ åœæ­¢æœåŠ¡

```bash
# åœæ­¢æœåŠ¡
docker compose down

# åœæ­¢å¹¶åˆ é™¤ç¼“å­˜ (é‡Šæ”¾ç£ç›˜ç©ºé—´)
docker compose down -v

# æ¸…ç†æœªä½¿ç”¨çš„ Docker èµ„æº
docker system prune -a
```

---

## ğŸ“š å®Œæ•´æ–‡æ¡£

- **éƒ¨ç½²æŒ‡å—**: [DEPLOYMENT.md](DEPLOYMENT.md)
- **é¡¹ç›® README**: [README.md](README.md)
- **è®¡åˆ’è¿½è¸ª**: [.temp/plan.md](.temp/plan.md)

---

## ğŸ†˜ éœ€è¦å¸®åŠ©ï¼Ÿ

**æ£€æŸ¥æ¸…å•**:
1. âœ… Docker æ­£åœ¨è¿è¡Œ
2. âœ… .env æ–‡ä»¶å·²é…ç½®
3. âœ… æ¨¡å‹å·²ä¸‹è½½
4. âœ… ç«¯å£æœªè¢«å ç”¨ (5173, 3000, 8000)

**æŸ¥çœ‹æ—¥å¿—**:
```bash
docker compose logs -f
```

**å¥åº·æ£€æŸ¥**:
```bash
curl http://localhost:8000/health
```

## ğŸ†˜ éœ€è¦å¸®åŠ©ï¼Ÿ

**æ£€æŸ¥æ¸…å•**:
1. âœ… Docker æ­£åœ¨è¿è¡Œ
2. âœ… Ollama æ­£åœ¨è¿è¡Œ (`ollama serve`)
3. âœ… å·²ä¸‹è½½æ¨¡å‹ (`ollama pull qwen2.5:0.5b`)
4. âœ… .env æ–‡ä»¶å·²é…ç½®
5. âœ… TTS/ASR æ¨¡å‹å·²ä¸‹è½½
6. âœ… ç«¯å£æœªè¢«å ç”¨ (5173, 3000, 8000, 11434)

**æŸ¥çœ‹æ—¥å¿—**:
```bash
# åç«¯æ—¥å¿—
docker compose logs -f backend

# Ollama æµ‹è¯•
curl http://localhost:11434/api/tags

# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health
```

**å¿«é€Ÿæµ‹è¯•**:
```bash
# æµ‹è¯• Ollama
ollama run qwen2.5:0.5b "ä½ å¥½"

# æŸ¥çœ‹èµ„æº
docker stats
```

---

## ğŸ“Š æ€§èƒ½æµ‹è¯•

```bash
# æŸ¥çœ‹å¥åº·çŠ¶æ€
curl http://localhost:8000/health

# æŸ¥çœ‹æ—¥å¿—
docker compose logs -f backend

# ç›‘æ§èµ„æº
docker stats
```

---

## ğŸ›‘ åœæ­¢æœåŠ¡

```bash
# åœæ­¢æœåŠ¡
docker compose down

# åœæ­¢ Ollama
# (å¦‚æœéœ€è¦) pkill ollama

# åœæ­¢å¹¶åˆ é™¤ç¼“å­˜ (é‡Šæ”¾ç£ç›˜ç©ºé—´)
docker compose down -v

# æ¸…ç†æœªä½¿ç”¨çš„ Docker èµ„æº
docker system prune -a
```
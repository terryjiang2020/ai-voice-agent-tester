import { useState, useEffect, useRef } from 'react'
import './App.css'
import { useVoiceChat } from './hooks/useVoiceChat'
import { sendVoiceStream } from './services/voiceStreamClient'
import { enqueueBase64PcmChunk, stopPlayback } from '../audio_converter.js'

function App() {
  const [isConnected, setIsConnected] = useState(false)
  const [transcript, setTranscript] = useState([])
  const [error, setError] = useState('')
  const [connectionStatus, setConnectionStatus] = useState('disconnected')
  const [orderInfo, setOrderInfo] = useState({ items: '', name: '', phone: '' })
  const [assistantTexts, setAssistantTexts] = useState({})
  const [selectedModel, setSelectedModel] = useState('local') // 'openai', 'grok', or 'local'
  const [micError, setMicError] = useState('')
  const [streamStatus, setStreamStatus] = useState('idle')
  const [streamLog, setStreamLog] = useState([])

  const peerConnectionRef = useRef(null)
  const dataChannelRef = useRef(null)
  const audioElementRef = useRef(null)
  const websocketRef = useRef(null)
  const audioContextRef = useRef(null)
  const audioQueueRef = useRef([])
  const isPlayingRef = useRef(false)
  const nextPlayTimeRef = useRef(0)
  const localServiceRef = useRef(null)
  const [useNewVoiceWS, setUseNewVoiceWS] = useState(false)
  const voiceHookRef = useRef(null)

  // VAD state management (using useRef to persist across re-renders)
  const vadStateRef = useRef({
    isSpeaking: false,
    speechStartTime: null,
    speechFrameCount: 0,
    silenceTimer: null,
    hasSentSpeechStart: false
  })
  const processorRef = useRef(null)

  // TTS audio queue management - using AudioWorklet for low-latency streaming
  const audioWorkletNodeRef = useRef(null)  // AudioWorkletNode for continuous playback
  const audioWorkletContextRef = useRef(null)  // AudioContext for AudioWorklet (24000 Hz)

  // Connect to selected model
  const connect = async () => {
    if (selectedModel === 'openai') {
      await connectOpenAI()
    } else if (selectedModel === 'grok') {
      await connectGrok()
    } else if (selectedModel === 'local') {
      // Initialize AudioWorklet for TTS before connecting
      await initializeAudioWorklet()
      await connectLocal()
    }
  }

  // Initialize AudioWorklet for low-latency TTS playback
  const initializeAudioWorklet = async () => {
    try {
      // CosyVoice outputs at 24000 Hz (per AUDIO_FORMAT_GUIDE.md)
      const COSYVOICE_SAMPLE_RATE = 24000
      
      const ctx = new AudioContext({ sampleRate: COSYVOICE_SAMPLE_RATE })
      audioWorkletContextRef.current = ctx

      console.log('[TTS] Initializing AudioWorklet with sample rate:', COSYVOICE_SAMPLE_RATE)

      // Load AudioWorklet processor module
      await ctx.audioWorklet.addModule('/pcm-player-processor.js')
      console.log('[TTS] AudioWorklet module loaded')

      // Create AudioWorkletNode
      const workletNode = new AudioWorkletNode(ctx, 'pcm-player-processor')
      workletNode.connect(ctx.destination)

      // Listen for status messages from worklet
      workletNode.port.onmessage = (event) => {
        const { type, queueLength, samplesReceived, samplesPlayed } = event.data
        if (type === 'status') {
          console.log(`[TTS Worklet] Queue: ${queueLength} samples, Received: ${samplesReceived}, Played: ${samplesPlayed}`)
        } else if (type === 'stopped') {
          console.log('[TTS Worklet] Playback stopped')
        }
      }

      audioWorkletNodeRef.current = workletNode
      console.log('[TTS] AudioWorklet initialized successfully')
    } catch (err) {
      console.error('[TTS] Failed to initialize AudioWorklet:', err)
      throw err
    }
  }

  // Initialize AudioWorklet for real-time TTS playback
  // CosyVoice outputs Float32 PCM at 24000 Hz (per AUDIO_FORMAT_GUIDE.md)
  const initializeAudioWorklet = async () => {
    try {
      // CosyVoice outputs at 24000 Hz
      const COSYVOICE_SAMPLE_RATE = 24000
      
      const ctx = new AudioContext({ sampleRate: COSYVOICE_SAMPLE_RATE })
      audioWorkletContextRef.current = ctx

      console.log('[TTS] Initializing AudioWorklet with sample rate:', COSYVOICE_SAMPLE_RATE)

      // Load AudioWorklet processor module
      await ctx.audioWorklet.addModule('/pcm-player-processor.js')
      console.log('[TTS] AudioWorklet module loaded')

      // Create AudioWorkletNode
      const workletNode = new AudioWorkletNode(ctx, 'pcm-player-processor')
      workletNode.connect(ctx.destination)

      // Listen for status messages from worklet
      workletNode.port.onmessage = (event) => {
        const { type, queueLength, samplesReceived, samplesPlayed } = event.data
        if (type === 'status') {
          console.log(`[TTS Worklet] Queue: ${queueLength} samples, Received: ${samplesReceived}, Played: ${samplesPlayed}`)
        } else if (type === 'stopped') {
          console.log('[TTS Worklet] Playback stopped')
        }
      }

      audioWorkletNodeRef.current = workletNode
      console.log('[TTS] AudioWorklet initialized successfully')
    } catch (err) {
      console.error('[TTS] Failed to initialize AudioWorklet:', err)
      throw err
    }
  }

  // Initialize AudioWorklet for real-time TTS playback
  const initAudioWorklet = async () => {
    try {
      console.log('[TTS] Initializing AudioWorklet...')

      // Create AudioContext if needed
      if (!audioContextRef.current || audioContextRef.current.state === 'closed') {
        const AudioContext = window.AudioContext || window.webkitAudioContext
        audioContextRef.current = new AudioContext({ sampleRate: 16000 })
        console.log('[TTS] Created AudioContext with sample rate: 16000')
      }

      const ctx = audioContextRef.current

      // Load AudioWorklet processor module
  // Stop TTS playback and clear worklet queue
  const stopTTSPlayback = () => {
    console.log('[TTS] Stopping playback and clearing queue')

    // Send stop message to AudioWorklet
    if (audioWorkletNodeRef.current) {
      audioWorkletNodeRef.current.port.postMessage({ type: 'stop' })
    }
  }

  // Process and stream TTS audio chunk to AudioWorklet
  // CosyVoice outputs Float32 PCM at 24000 Hz (per AUDIO_FORMAT_GUIDE.md)
  const processTTSChunk = (base64Audio) => {
    try {
      // 1) Base64 -> binary bytes
      const binary = atob(base64Audio)
      const bytes = new Uint8Array(binary.length)
      for (let i = 0; i < binary.length; i++) {
        bytes[i] = binary.charCodeAt(i)
      }

      console.log(`[TTS] Decoded ${bytes.length} bytes`)

      // 2) Convert bytes to Float32Array (CosyVoice native format)
      // Each Float32 sample is 4 bytes
      if (bytes.length % 4 !== 0) {
        console.warn(`[TTS] Warning: byte length ${bytes.length} not divisible by 4`)
      }

      const float32Data = new Float32Array(
        bytes.buffer,
        bytes.byteOffset,
        Math.floor(bytes.length / 4)
      )

      console.log(`[TTS] Converted to ${float32Data.length} Float32 samples (range check: ${float32Data[0]?.toFixed(4)})`)

      // 3) Send to AudioWorklet for immediate playback
      if (audioWorkletNodeRef.current) {
        audioWorkletNodeRef.current.port.postMessage({
          type: 'pcm',
          samples: float32Data
        })
        console.log(`[TTS] Sent ${float32Data.length} samples to AudioWorklet`)
      } else {
        console.error('[TTS] AudioWorklet not initialized')
      }
    } catch (err) {
      console.error('[TTS] Error processing audio chunk:', err)
    }
  }

  const playTTSQueueAll = () => {odule('/pcm-player-processor.js')
      console.log('[TTS] AudioWorklet module loaded')

      // Create AudioWorkletNode
      const workletNode = new AudioWorkletNode(ctx, 'pcm-player-processor')
      workletNode.connect(ctx.destination)

      // Listen for status messages from worklet
      workletNode.port.onmessage = (event) => {
        const { type, queueLength, samplesReceived, samplesPlayed } = event.data
        if (type === 'status') {
          console.log(`[TTS Worklet] Queue: ${queueLength} samples, Received: ${samplesReceived}, Played: ${samplesPlayed}`)
        } else if (type === 'stopped') {
          console.log('[TTS Worklet] Playback stopped')
        }
      }

      audioWorkletNodeRef.current = workletNode
      console.log('[TTS] AudioWorklet initialized successfully')
    } catch (err) {
      console.error('[TTS] Failed to initialize AudioWorklet:', err)
      throw err
    }
  }

  // Stop TTS playback and clear queue
  const stopTTSPlayback = () => {
    console.log('[TTS] Stopping playback and clearing queue')

    // Clear main thread queue
    ttsQueueRef.current = []

    // Send stop message to AudioWorklet
    if (audioWorkletNodeRef.current) {
      audioWorkletNodeRef.current.port.postMessage({ type: 'stop' })
    }
  }

  const playTTSQueueAll = () => {
    console.log('[TTS] Playing all queued audio chunks')

    if (ttsQueueRef.current.length === 0) {
      console.log('[TTS] Queue empty')
      return
    }

    // Collect all audio chunks
    const chunks = []
    while (ttsQueueRef.current.length > 0) {
      chunks.push(ttsQueueRef.current.shift())
    }

    console.log(`[TTS] Processing ${chunks.length} chunks`)

    // Process each chunk with format detection and alignment
    chunks.forEach((audioChunk, index) => {
      try {
        // 1) Base64 ‚Üí Uint8Array
        let pcmBytes = Uint8Array.from(atob(audioChunk.audio), c => c.charCodeAt(0))
        console.log(`[TTS] Chunk ${index + 1}: Decoded ${pcmBytes.length} bytes`)

        // 2) Auto-detect and fix format
        let float32Samples

        if (pcmBytes.length % 4 === 0) {
          // Perfect Float32 alignment
          console.log(`[TTS] Chunk ${index + 1}: Float32 PCM (perfect alignment)`)
          float32Samples = new Float32Array(pcmBytes.buffer, pcmBytes.byteOffset, pcmBytes.length / 4)
        } else if (pcmBytes.length % 2 === 0) {
          // Int16 format
          console.log(`[TTS] Chunk ${index + 1}: Int16 PCM`)
          const int16Samples = new Int16Array(pcmBytes.buffer, pcmBytes.byteOffset, pcmBytes.length / 2)
          float32Samples = new Float32Array(int16Samples.length)
          for (let i = 0; i < int16Samples.length; i++) {
            float32Samples[i] = int16Samples[i] / (int16Samples[i] < 0 ? 32768 : 32767)
          }
        } else {
          // Misaligned - try to fix
          console.warn(`[TTS] Chunk ${index + 1}: Misaligned data: ${pcmBytes.length} bytes`)

          // Try trimming 1-3 bytes to achieve 4-byte alignment (Float32)
          let trimmed = 0
          for (let trim = 1; trim <= 3; trim++) {
            if ((pcmBytes.length - trim) % 4 === 0) {
              trimmed = trim
              break
            }
          }

          if (trimmed > 0) {
            console.log(`[TTS] Chunk ${index + 1}: Trimming last ${trimmed} byte(s) for alignment`)
            pcmBytes = pcmBytes.slice(0, pcmBytes.length - trimmed)
            float32Samples = new Float32Array(pcmBytes.buffer, pcmBytes.byteOffset, pcmBytes.length / 4)
          } else {
            throw new Error(`Cannot align ${pcmBytes.length} bytes to any audio format`)
          }
        }

        console.log(`[TTS] Chunk ${index + 1}: Sending ${float32Samples.length} samples (first: ${float32Samples[0].toFixed(6)})`)

        // 3) Send to AudioWorklet for real-time playback
        if (audioWorkletNodeRef.current) {
          audioWorkletNodeRef.current.port.postMessage({
            type: 'pcm',
            samples: float32Samples
          })
        }
      } catch (err) {
        console.error(`[TTS] Error processing chunk ${index + 1}:`, err)
      }
    })

    console.log('[TTS] All chunks sent to AudioWorklet')
  }


  // Play TTS audio queue (FIFO) - Real-time streaming with AudioWorklet
  const playTTSQueue = () => {
    if (ttsQueueRef.current.length === 0) {
      console.log('[TTS] Queue empty')
      return
    }

    // Process all chunks in queue
    while (ttsQueueRef.current.length > 0) {
      const audioChunk = ttsQueueRef.current.shift()
      console.log(`[TTS] Processing chunk, ${ttsQueueRef.current.length} remaining in queue`)

      try {
        // 1) Base64 ‚Üí Uint8Array
        let pcmBytes = Uint8Array.from(atob(audioChunk.audio), c => c.charCodeAt(0))
        console.log(`[TTS] Decoded ${pcmBytes.length} bytes`)

        // 2) Auto-detect and fix format
        let float32Samples;

        if (pcmBytes.length % 4 === 0) {
          // Perfect Float32 alignment
          console.log('[TTS] Format: Float32 PCM (perfect alignment)')
          float32Samples = new Float32Array(pcmBytes.buffer, pcmBytes.byteOffset, pcmBytes.length / 4)
        } else if (pcmBytes.length % 2 === 0) {
          // Int16 format
          console.log('[TTS] Format: Int16 PCM')
          const int16Samples = new Int16Array(pcmBytes.buffer, pcmBytes.byteOffset, pcmBytes.length / 2)
          float32Samples = new Float32Array(int16Samples.length)
          for (let i = 0; i < int16Samples.length; i++) {
            float32Samples[i] = int16Samples[i] / (int16Samples[i] < 0 ? 32768 : 32767)
          }
        } else {
          // Misaligned - try to fix
          console.warn(`[TTS] Misaligned data: ${pcmBytes.length} bytes (not divisible by 2 or 4)`)

          // Try trimming 1-3 bytes to achieve 4-byte alignment (Float32)
          let trimmed = 0
          for (let trim = 1; trim <= 3; trim++) {
            if ((pcmBytes.length - trim) % 4 === 0) {
              trimmed = trim
              break
            }
          }

          if (trimmed > 0) {
            console.log(`[TTS] Trimming last ${trimmed} byte(s) for alignment`)
            pcmBytes = pcmBytes.slice(0, pcmBytes.length - trimmed)
            float32Samples = new Float32Array(pcmBytes.buffer, pcmBytes.byteOffset, pcmBytes.length / 4)
          } else {
            throw new Error(`Cannot align ${pcmBytes.length} bytes to any audio format`)
          }
        }

        // Validate Float32 range
        const sample = float32Samples[0]
        if (Math.abs(sample) > 100) {
          console.warn(`[TTS] Suspicious Float32 value: ${sample} (may be wrong format)`)
        }

        console.log(`[TTS] Sending ${float32Samples.length} samples to AudioWorklet (first sample: ${float32Samples[0].toFixed(6)})`)

        // 3) Send to AudioWorklet for real-time playback
        if (audioWorkletNodeRef.current) {
          audioWorkletNodeRef.current.port.postMessage({
            type: 'pcm',
            samples: float32Samples
          })
        }
      } catch (err) {
        console.error('[TTS] Error processing audio chunk:', err)
      }
    }
  }

  const connectOpenAI = async () => {
    try {
      setConnectionStatus('connecting')
      setError('')

      // Step 1: Get ephemeral token from our server
      console.log('Fetching ephemeral token from server...')
      const tokenResponse = await fetch('http://localhost:3000/token?model=openai')
      if (!tokenResponse.ok) {
        throw new Error(`Failed to fetch ephemeral token: ${tokenResponse.statusText}`)
      }
      const data = await tokenResponse.json()
      const EPHEMERAL_KEY = data.value

      console.log('Ephemeral token received, setting up WebRTC...')

      // Step 2: Create a peer connection
      const pc = new RTCPeerConnection()
      peerConnectionRef.current = pc

      // Step 3: Set up to play remote audio from the model
      if (!audioElementRef.current) {
        audioElementRef.current = document.createElement('audio')
        audioElementRef.current.autoplay = true
        audioElementRef.current.volume = 1.0
      }
      pc.ontrack = (e) => {
        console.log('Received audio track from server')
        audioElementRef.current.srcObject = e.streams[0]
        audioElementRef.current.play().catch(err => console.log('Audio play error:', err))
      }

      // Step 4: Add local audio track for microphone input
      console.log('Requesting microphone access...')
      const ms = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 24000,
          channelCount: 1
        },
      })
      pc.addTrack(ms.getTracks()[0])

      // Step 5: Set up data channel for sending and receiving events
      const dc = pc.createDataChannel('oai-events')
      dataChannelRef.current = dc

      dc.onopen = () => {
        console.log('Data channel opened - connected!')
        setIsConnected(true)
        setConnectionStatus('connected')
      }

      dc.onclose = () => {
        console.log('Data channel closed')
        setIsConnected(false)
        setConnectionStatus('disconnected')
      }

      dc.onerror = (err) => {
        console.error('Data channel error:', err)
        setError(`Data channel error: ${err}`)
      }

      // Listen for server events
      dc.addEventListener('message', (e) => {
        const event = JSON.parse(e.data)
        console.log('Received event:', event.type)
        console.log('Event data:', event)

        switch (event.type) {
          case 'conversation.item.added':
            if (event.item?.type === 'message') {
              const role = event.item.role
              const content = event.item.content?.[0]
              console.log('Message role:', role, 'content:', content, 'content array length:', event.item.content?.length)
              if (role === 'user' && content?.transcript) {
                addTranscript(role, content.transcript)
              }
              // For assistant, content is empty array initially, we'll accumulate from deltas
            }
            break

          case 'response.output_audio_transcript.delta':
            if (event.delta) {
              const responseId = event.response_id
              setAssistantTexts(prev => ({
                ...prev,
                [responseId]: (prev[responseId] || '') + event.delta
              }))
              console.log(`Delta for ${responseId}:`, event.delta)
            }
            break

          case 'response.output_audio_transcript.done':
            const responseId = event.response_id
            const finalText = assistantTexts[responseId] || ''
            console.log(`Done for ${responseId}, final text:`, finalText)
            if (finalText) {
              addTranscript('assistant', finalText)
              setAssistantTexts(prev => {
                const newTexts = { ...prev }
                delete newTexts[responseId]
                return newTexts
              })
            }
            break

          case 'conversation.item.input_audio_transcription.completed':
            if (event.transcript) {
              addTranscript('user', event.transcript)
            }
            break

          case 'error':
            console.error('API Error:', event.error)
            setError(`API Error: ${event.error.message}`)
            break
        }
      })

      // Step 6: Start the session using the Session Description Protocol (SDP)
      const offer = await pc.createOffer()
      await pc.setLocalDescription(offer)

      console.log('Connecting to OpenAI Realtime API...')
      const sdpResponse = await fetch('https://api.openai.com/v1/realtime/calls', {
        method: 'POST',
        body: offer.sdp,
        headers: {
          Authorization: `Bearer ${EPHEMERAL_KEY}`,
          'Content-Type': 'application/sdp',
        },
      })

      if (!sdpResponse.ok) {
        throw new Error(`SDP exchange failed: ${sdpResponse.statusText}`)
      }

      const answer = {
        type: 'answer',
        sdp: await sdpResponse.text(),
      }
      await pc.setRemoteDescription(answer)

      console.log('WebRTC connection established successfully')
    } catch (err) {
      console.error('Connection error:', err)
      setError(`Failed to connect: ${err.message}`)
      setConnectionStatus('disconnected')
    }
  }

  // Disconnect from the API
  const disconnect = () => {
    // Close local service
    if (localServiceRef.current) {
      localServiceRef.current.disconnect()
      localServiceRef.current = null
    }

    // Close WebSocket if using Grok
    if (websocketRef.current) {
      websocketRef.current.close()
      websocketRef.current = null
    }

    // Clean up audio processor (for local model VAD)
    if (processorRef.current) {
      console.log('[Cleanup] Disconnecting audio processor')
      processorRef.current.disconnect()
      processorRef.current = null
    }

    // Close AudioContext if using Grok or Local
    if (audioContextRef.current) {
      audioContextRef.current.close()
      audioContextRef.current = null
    }

    // Clean up AudioWorklet
    if (audioWorkletNodeRef.current) {
      audioWorkletNodeRef.current.disconnect()
      audioWorkletNodeRef.current = null
    }
    if (audioWorkletContextRef.current) {
      audioWorkletContextRef.current.close()
      audioWorkletContextRef.current = null
    }

    stopPlayback()

    // Clear audio queue
    audioQueueRef.current = []
    isPlayingRef.current = false
    nextPlayTimeRef.current = 0

    // Reset VAD state
    if (vadStateRef.current.silenceTimer) {
      clearTimeout(vadStateRef.current.silenceTimer)
    }
    vadStateRef.current = {
      isSpeaking: false,
      speechStartTime: null,
      speechFrameCount: 0,
      silenceTimer: null,
      hasSentSpeechStart: false
    }

    // Stop TTS playback and clear queue
    stopTTSPlayback()

    // Clean up AudioWorklet
    if (audioWorkletNodeRef.current) {
      console.log('[Cleanup] Disconnecting AudioWorklet')
      audioWorkletNodeRef.current.disconnect()
      audioWorkletNodeRef.current = null
    }

    // Close WebRTC if using OpenAI
    if (dataChannelRef.current) {
      dataChannelRef.current.close()
      dataChannelRef.current = null
    }

    if (peerConnectionRef.current) {
      peerConnectionRef.current.close()
      peerConnectionRef.current = null
    }

    if (audioElementRef.current) {
      audioElementRef.current.srcObject = null
    }

    setIsConnected(false)
    setConnectionStatus('disconnected')
  }

  const appendStreamLog = (msg) => {
    setStreamLog((prev) => [...prev.slice(-20), { msg, ts: new Date().toLocaleTimeString() }])
  }

  const recordOnce = async (durationMs = 2000) => {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
    const recorder = new MediaRecorder(stream, { mimeType: 'audio/webm' })
    const chunks = []
    return new Promise((resolve, reject) => {
      recorder.ondataavailable = (e) => { if (e.data.size > 0) chunks.push(e.data) }
      recorder.onerror = reject
      recorder.onstop = async () => {
        const blob = new Blob(chunks, { type: 'audio/webm' })
        const arrayBuffer = await blob.arrayBuffer()
        resolve(arrayBuffer)
      }
      recorder.start(250)
      setTimeout(() => recorder.stop(), durationMs)
    })
  }

  const runUnifiedStream = async () => {
    try {
      setStreamStatus('recording')
      appendStreamLog('Recording mic for 2s...')
      const audioBuffer = await recordOnce(2000)
      setStreamStatus('streaming')
      appendStreamLog('Sending to /api/voice/stream ...')

      await sendVoiceStream(audioBuffer, {
        onFrame: (frame) => {
          if (frame.type === 'asr') {
            appendStreamLog(`ASR: ${frame.text}`)
            addTranscript('user', frame.text)
          } else if (frame.type === 'llm') {
            appendStreamLog(`LLM${frame.partial ? ' partial' : ' final'}: ${frame.text}`)
            if (!frame.partial) addTranscript('assistant', frame.text)
          } else if (frame.type === 'tts') {
            enqueueBase64PcmChunk(frame.audio)
          } else if (frame.type === 'done') {
            appendStreamLog('Stream done')
          } else if (frame.type === 'error') {
            appendStreamLog(`Error: ${frame.message}`)
            setError(frame.message || 'Stream error')
          }
        },
      })

      setStreamStatus('done')
    } catch (e) {
      setStreamStatus('error')
      setError(e.message)
      appendStreamLog(`Error: ${e.message}`)
    }
  }

  // Play audio queue smoothly without gaps
  const playAudioQueue = (audioContext) => {
    if (audioQueueRef.current.length === 0) {
      isPlayingRef.current = false
      return
    }

    isPlayingRef.current = true
    const float32 = audioQueueRef.current.shift()

    // Create audio buffer
    const audioBuffer = audioContext.createBuffer(1, float32.length, 24000)
    audioBuffer.getChannelData(0).set(float32)
    
    const bufferSource = audioContext.createBufferSource()
    bufferSource.buffer = audioBuffer
    bufferSource.connect(audioContext.destination)

    // Calculate when to start this chunk
    const currentTime = audioContext.currentTime
    const startTime = Math.max(currentTime, nextPlayTimeRef.current)
    
    bufferSource.start(startTime)
    
    // Update next play time to ensure continuous playback
    nextPlayTimeRef.current = startTime + audioBuffer.duration

    // When this chunk finishes, play the next one
    bufferSource.onended = () => {
      playAudioQueue(audioContext)
    }
  }

  // Connect to local model service
  const connectLocal = async () => {
    try {
      setConnectionStatus('connecting')
      setError('')

      // Clean up old processor and reset VAD state before connecting
      if (processorRef.current) {
        console.log('[connectLocal] Cleaning up old processor')
        processorRef.current.disconnect()
        processorRef.current = null
      }
      if (vadStateRef.current.silenceTimer) {
        clearTimeout(vadStateRef.current.silenceTimer)
      }
      vadStateRef.current = {
        isSpeaking: false,
        speechStartTime: null,
        speechFrameCount: 0,
        silenceTimer: null,
        hasSentSpeechStart: false
      }

      // Step 1: Create WebSocket connection
      const ws = new WebSocket('wss://devserver.elasticdash.com/ws/voice/stream')
      websocketRef.current = ws

      let reconnectAttempts = 0;
      const maxReconnectAttempts = 5;

      const attemptReconnect = () => {
        if (reconnectAttempts < maxReconnectAttempts) {
          reconnectAttempts++;
          console.log(`Reconnecting... Attempt ${reconnectAttempts}`);
          setTimeout(connectLocal, 2000); // Retry after 2 seconds
        } else {
          console.error("Max reconnect attempts reached");
          setError("Unable to reconnect after multiple attempts");
        }
      };

      ws.onopen = async () => {
        console.log('WebSocket connected');
        setIsConnected(true); // Ensure isConnected is updated
        setConnectionStatus('connected'); // Ensure connectionStatus is updated
        console.log('Updated states: isConnected = true, connectionStatus = connected');

        // Initialize AudioWorklet for TTS playback
        try {
          await initAudioWorklet();
        } catch (err) {
          console.error('[WebSocket.onopen] Failed to initialize AudioWorklet:', err);
        }
      }

      ws.onmessage = (event) => {
        const message = JSON.parse(event.data)
        console.log('Received event:', message)

        switch (message.type) {
          case 'asr.partial':
            console.log('[ASR Partial] Text:', message.text);
            // Update the same message until we get final
            updateLastTranscript('user', message.text, true);
            break;
          case 'asr.final':
            console.log('[ASR Final] Text:', message.text);
            // Finalize the transcript message
            updateLastTranscript('user', message.text, false);
            break;
          case 'response.output_audio_transcript.delta':
            console.log('[AI Delta] Text:', message.text);
            // Update AI response (partial)
            updateLastTranscript('AI', message.text, true);
            break;
          case 'response.output_audio_transcript.done':
            console.log('[AI Done] Text:', message.text);
            // Finalize AI response
            updateLastTranscript('AI', message.text, false);
            break;
          case 'error':
            console.error('[WebSocket Error] Message:', message.message);
            setError(message.message)
            break
          case 'done':
            console.log('Stream complete')
            playTTSQueueAll();
            break
          case 'tts.chunk':
            console.log('[TTS Chunk] Received audio chunk, sample_rate:', message.sample_rate);
            // Add audio chunk to queue
            ttsQueueRef.current.push({
              audio: message.audio,
              sample_rate: message.sample_rate || 16000
            });
            console.log(`[TTS] Queue size: ${ttsQueueRef.current.length}`);
            // Start playing if not already playing
            // playTTSQueue();
            break
          case 'llm.partial':
            console.log('[LLM Partial] Text:', message.text);
            if (message.text) {
              setTranscript((prev) => {
                const newTranscript = [...prev];
                const lastIndex = newTranscript.length - 1;

                // Check if the last message is a partial LLM message
                if (
                  lastIndex >= 0 &&
                  newTranscript[lastIndex].role === 'assistant' &&
                  newTranscript[lastIndex].isPartial
                ) {
                  // Concatenate the new character with the existing partial text
                  newTranscript[lastIndex] = {
                    ...newTranscript[lastIndex],
                    text: newTranscript[lastIndex].text + message.text,
                    timestamp: new Date().toLocaleTimeString(),
                  };
                } else {
                  // Add a new partial message
                  newTranscript.push({
                    role: 'assistant',
                    text: message.text,
                    isPartial: true,
                    timestamp: new Date().toLocaleTimeString(),
                  });
                }

                return newTranscript;
              });
            }
            break;
          case 'llm.final':
            console.log('[LLM Final] Text:', message.text);
            if (message.text) {
              setTranscript((prev) => {
                const newTranscript = [...prev];
                const lastIndex = newTranscript.length - 1;

                // Check if the last message is a partial LLM message
                if (
                  lastIndex >= 0 &&
                  newTranscript[lastIndex].role === 'assistant' &&
                  newTranscript[lastIndex].isPartial
                ) {
                  // Finalize the existing partial message
                  newTranscript[lastIndex] = {
                    ...newTranscript[lastIndex],
                    text: message.text,
                    isPartial: false,
                    timestamp: new Date().toLocaleTimeString(),
                  };
                } else {
                  // Add a new finalized message
                  newTranscript.push({
                    role: 'assistant',
                    text: message.text,
                    isPartial: false,
                    timestamp: new Date().toLocaleTimeString(),
                  });
                }

                return newTranscript;
              });
            }
            break;
        }
      }

      ws.onclose = () => {
        console.log('WebSocket disconnected')
        setConnectionStatus('disconnected')

        // Clean up processor when WebSocket closes to prevent errors
        if (processorRef.current) {
          console.log('[WebSocket.onclose] Cleaning up processor')
          processorRef.current.disconnect()
          processorRef.current = null
        }
        if (audioContextRef.current) {
          audioContextRef.current.close()
          audioContextRef.current = null
        }

        attemptReconnect();
      }

      ws.onerror = (err) => {
        console.error('WebSocket error:', err)
        setError('WebSocket connection error')
        setConnectionStatus('disconnected')

        // Clean up processor on WebSocket error
        if (processorRef.current) {
          console.log('[WebSocket.onerror] Cleaning up processor')
          processorRef.current.disconnect()
          processorRef.current = null
        }
        if (audioContextRef.current) {
          audioContextRef.current.close()
          audioContextRef.current = null
        }

        attemptReconnect();
      }

      // Step 2: Start streaming audio with VAD
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      const audioContext = new AudioContext({ sampleRate: 16000 })
      const source = audioContext.createMediaStreamSource(stream)
      const processor = audioContext.createScriptProcessor(4096, 1, 1)

      audioContextRef.current = audioContext
      processorRef.current = processor

      // VAD (Voice Activity Detection) constants
      const SILENCE_THRESHOLD = 0.03 // Balanced threshold (0.02-0.05 recommended)
      const SILENCE_TIMEOUT = 2000 // 2 seconds of silence before ending speech
      const MIN_SPEECH_DURATION = 1000 // Minimum 1 second of speech
      const MIN_SPEECH_FRAMES = 3 // Need 3 consecutive frames to start (avoid noise)

      // Note: ScriptProcessorNode.onaudioprocess is deprecated.
      // Consider migrating to AudioWorklet for better performance in the future.
      processor.onaudioprocess = (event) => {
        // Skip processing if WebSocket is not open
        if (ws.readyState !== WebSocket.OPEN) {
          return;
        }

        const audioData = event.inputBuffer.getChannelData(0);
        const vad = vadStateRef.current;

        // Calculate RMS (Root Mean Square) to determine volume level
        let sum = 0;
        for (let i = 0; i < audioData.length; i++) {
          sum += audioData[i] * audioData[i];
        }
        const rms = Math.sqrt(sum / audioData.length);

        if (rms > SILENCE_THRESHOLD) {
          // Potential speech detected
          if (!vad.isSpeaking) {
            vad.speechFrameCount++;
            if (vad.speechFrameCount >= MIN_SPEECH_FRAMES) {
              vad.isSpeaking = true;
              vad.speechStartTime = Date.now();
              vad.speechFrameCount = 0;

              // Only send speech.start once per speech session
              if (!vad.hasSentSpeechStart) {
                vad.hasSentSpeechStart = true;
                console.log('[VAD] Speech started, RMS:', rms.toFixed(4));

                // Stop TTS playback when user starts speaking (interruption)
                stopTTSPlayback();

                // Notify backend that speech started
                if (ws.readyState === WebSocket.OPEN) {
                  ws.send(JSON.stringify({ type: 'speech.start' }));
                } else {
                  console.error('[WebSocket] Cannot send speech.start, WebSocket not open');
                }
              }
            }
          } else {
            vad.speechFrameCount = 0;
          }

          if (vad.silenceTimer) {
            clearTimeout(vad.silenceTimer);
            vad.silenceTimer = null;
          }

          // Send audio frame
          const int16Array = new Int16Array(audioData.length);
          for (let i = 0; i < audioData.length; i++) {
            int16Array[i] = Math.min(1, Math.max(-1, audioData[i])) * 32767;
          }

          if (ws.readyState === WebSocket.OPEN) {
            ws.send(int16Array.buffer);
          } else {
            console.error('[WebSocket] Cannot send audio frame, WebSocket not open');
          }
        } else {
          // Silence detected
          if (!vad.isSpeaking) {
            vad.speechFrameCount = 0;
          } else {
            const speechDuration = Date.now() - vad.speechStartTime;

            if (speechDuration >= MIN_SPEECH_DURATION) {
              if (!vad.silenceTimer) {
                vad.silenceTimer = setTimeout(() => {
                  console.log('[VAD] Speech ended after silence timeout');

                  // Reset VAD state
                  vad.isSpeaking = false;
                  vad.silenceTimer = null;
                  vad.speechStartTime = null;
                  vad.speechFrameCount = 0;
                  vad.hasSentSpeechStart = false; // Reset for next speech session

                  // Notify backend that speech ended
                  if (ws.readyState === WebSocket.OPEN) {
                    ws.send(JSON.stringify({ type: 'speech.end' }));
                  } else {
                    console.error('[WebSocket] Cannot send speech.end, WebSocket not open');
                  }
                }, SILENCE_TIMEOUT);
              }
            }
          }
        }
      }

      source.connect(processor)
      processor.connect(audioContext.destination)

      console.log('Audio streaming started with VAD')
    } catch (err) {
      console.error('Connection error:', err)
      setError(`Failed to connect: ${err.message}`)
      setConnectionStatus('disconnected')
    }
  }

  const connectGrok = async () => {
    try {
      setConnectionStatus('connecting')
      setError(null)

      // Reset audio queue
      audioQueueRef.current = []
      isPlayingRef.current = false
      nextPlayTimeRef.current = 0

      // Get ephemeral token for Grok
      const tokenResponse = await fetch('http://localhost:3000/token?model=grok')
      if (!tokenResponse.ok) {
        throw new Error('Failed to get Grok ephemeral token')
      }
      const { token } = await tokenResponse.json()

      // Create WebSocket connection
      const ws = new WebSocket(
        `wss://api.x.ai/v1/realtime?model=grok-2-latest`,
        ['realtime', `openai-insecure-api-key.${token}`, 'openai-beta.realtime-v1']
      )
      websocketRef.current = ws

      // Create AudioContext for audio processing
      const audioContext = new AudioContext({ sampleRate: 24000 })
      audioContextRef.current = audioContext

      ws.onopen = async () => {
        console.log('Grok WebSocket connected')
        
        // Send session configuration
        ws.send(JSON.stringify({
          type: 'session.update',
          session: {
            voice: 'Ara',
            instructions: formatter,
            input_audio_format: 'pcm16',
            output_audio_format: 'pcm16',
            input_audio_transcription: {
              model: 'whisper-1'
            },
            turn_detection: {
              type: 'server_vad',
              threshold: 0.5,
              prefix_padding_ms: 300,
              silence_duration_ms: 500
            }
          }
        }))

        // Start capturing microphone audio
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            channelCount: 1,
            sampleRate: 24000,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        })

        const source = audioContext.createMediaStreamSource(stream)
        const processor = audioContext.createScriptProcessor(4096, 1, 1)

        processor.onaudioprocess = (e) => {
          if (ws.readyState === WebSocket.OPEN) {
            const inputData = e.inputBuffer.getChannelData(0)
            // Convert Float32Array to Int16Array
            const pcm16 = new Int16Array(inputData.length)
            for (let i = 0; i < inputData.length; i++) {
              const s = Math.max(-1, Math.min(1, inputData[i]))
              pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF
            }
            // Convert to base64
            const base64 = btoa(String.fromCharCode(...new Uint8Array(pcm16.buffer)))
            ws.send(JSON.stringify({
              type: 'input_audio_buffer.append',
              audio: base64
            }))
          }
        }

        source.connect(processor)
        processor.connect(audioContext.destination)

        setIsConnected(true)
        setConnectionStatus('connected')
      }

      ws.onmessage = (event) => {
        const message = JSON.parse(event.data)
        console.log('Grok message:', message.type)

        if (message.type === 'session.updated') {
          console.log('Session configured:', message.session)
        } else if (message.type === 'input_audio_buffer.speech_started') {
          console.log('User started speaking')
        } else if (message.type === 'input_audio_buffer.speech_stopped') {
          console.log('User stopped speaking')
        } else if (message.type === 'conversation.item.input_audio_transcription.completed') {
          const userText = message.transcript
          console.log('User said:', userText)
          addTranscript('user', userText)
        } else if (message.type === 'response.output_audio_transcript.delta') {
          const responseId = message.response_id
          if (!assistantTextsRef.current[responseId]) {
            assistantTextsRef.current[responseId] = ''
          }
          assistantTextsRef.current[responseId] += message.delta
          console.log('Accumulating AI text:', assistantTextsRef.current[responseId])
        } else if (message.type === 'response.output_audio_transcript.done') {
          const responseId = message.response_id
          const fullText = assistantTextsRef.current[responseId] || message.transcript
          if (fullText) {
            console.log('AI response complete:', fullText)
            addTranscript('AI', fullText)
            delete assistantTextsRef.current[responseId]
          }
        } else if (message.type === 'response.output_audio.delta') {
          // Queue audio for smooth playback
          if (message.delta && audioContext) {
            try {
              // Decode base64 to PCM16
              const binaryString = atob(message.delta)
              const bytes = new Uint8Array(binaryString.length)
              for (let i = 0; i < binaryString.length; i++) {
                bytes[i] = binaryString.charCodeAt(i)
              }
              const pcm16 = new Int16Array(bytes.buffer)
              
              // Convert Int16Array to Float32Array
              const float32 = new Float32Array(pcm16.length)
              for (let i = 0; i < pcm16.length; i++) {
                float32[i] = pcm16[i] / (pcm16[i] < 0 ? 0x8000 : 0x7FFF)
              }

              // Add to queue and start playing if not already
              audioQueueRef.current.push(float32)
              if (!isPlayingRef.current) {
                playAudioQueue(audioContext)
              }
            } catch (err) {
              console.error('Error decoding audio:', err)
            }
          }
        } else if (message.type === 'response.output_audio.done') {
          console.log('Audio response complete')
        } else if (message.type === 'error') {
          console.error('Grok error:', message.error)
          setError(`Grok error: ${message.error.message}`)
        }
      }

      ws.onerror = (error) => {
        console.error('WebSocket error:', error)
        setError('WebSocket connection error')
        setConnectionStatus('disconnected')
      }

      ws.onclose = () => {
        console.log('Grok WebSocket closed')
        setIsConnected(false)
        setConnectionStatus('disconnected')
        if (audioContext) {
          audioContext.close()
        }
      }

    } catch (err) {
      console.error('Grok connection error:', err)
      setError(`Failed to connect to Grok: ${err.message}`)
      setConnectionStatus('disconnected')
    }
  }

  // Update the last transcript message (for partial updates)
  const updateLastTranscript = (role, text, isPartial = false) => {
    console.log('updateLastTranscript called with role:', role, 'text:', text, 'isPartial:', isPartial)
    if (!text || text.trim() === '') {
      console.log('Skipping empty text')
      return
    }
    setTranscript(prev => {
      const newTranscript = [...prev]
      // Find the last message with the same role
      const lastIndex = newTranscript.length - 1
      if (lastIndex >= 0 && newTranscript[lastIndex].role === role && newTranscript[lastIndex].isPartial) {
        // Update existing partial message
        newTranscript[lastIndex] = {
          ...newTranscript[lastIndex],
          text,
          isPartial,
          timestamp: new Date().toLocaleTimeString()
        }
      } else {
        // Add new message
        newTranscript.push({
          role,
          text,
          isPartial,
          timestamp: new Date().toLocaleTimeString()
        })
      }
      console.log('Updated transcript:', newTranscript)
      return newTranscript
    })
  }

  // Add message to transcript & extract order info
  const addTranscript = (role, text) => {
    console.log('addTranscript called with role:', role, 'text:', text) // Ë∞ÉËØïÊó•Âøó
    if (!text || text.trim() === '') {
      console.log('Skipping empty text')
      return
    }
    setTranscript(prev => {
      const newTranscript = [...prev, { role, text, isPartial: false, timestamp: new Date().toLocaleTimeString() }]
      console.log('New transcript:', newTranscript) // Ë∞ÉËØïÊó•Âøó
      return newTranscript
    })
    if (role === 'user' || role === 'assistant' || role === 'AI') {
      console.log('Processing for order info extraction with text:', text) // Ë∞ÉËØïÊó•Âøó
      let items = orderInfo.items
      let name = orderInfo.name
      let phone = orderInfo.phone

      // ËèúÂìÅ - Êõ¥ÂÆΩÊ≥õÁöÑÂåπÈÖç
      const itemPatterns = [
        /(ÁÇπ‰∫Ü|ÈÄâÊã©‰∫Ü|ËèúÂìÅ|ËÆ¢Âçï|order|items?|ÊÉ≥Ë¶Å|Ë¶ÅÂêÉ)[:Ôºö]?\s*([\w\W]{2,50}?)(?=Ôºå|„ÄÇ|,|\n|$|ÂßìÂêç|ÁîµËØù|name|phone)/i,
        /ÁÇπÈ§ê[:Ôºö]?\s*([\w\W]{2,50}?)(?=Ôºå|„ÄÇ|,|\n|$)/i
      ]
      for (const pattern of itemPatterns) {
        const match = text.match(pattern)
        if (match && match[1]) {
          items = match[1].trim()
          console.log('Found items:', items) // Ë∞ÉËØïÊó•Âøó
          break
        }
      }

      // ÂßìÂêç - Êõ¥ÂÆΩÊ≥õÁöÑÂåπÈÖç
      const namePatterns = [
        /(ÂßìÂêç|name|order for|È°æÂÆ¢Âêç|ÂêçÂ≠ó|Âè´|ÊàëÊòØ)[:Ôºö]?\s*([\w\u4e00-\u9fa5]{1,20})/i,
        /ÊàëÂè´([\w\u4e00-\u9fa5]{1,20})/i
      ]
      for (const pattern of namePatterns) {
        const match = text.match(pattern)
        if (match && match[2]) {
          name = match[2].trim()
          console.log('Found name:', name) // Ë∞ÉËØïÊó•Âøó
          break
        }
      }

      // ÁîµËØù - Êõ¥ÂÆΩÊ≥õÁöÑÂåπÈÖç
      const phonePatterns = [
        /(ÁîµËØù|phone|number|ÊâãÊú∫Âè∑|ËÅîÁ≥ªÊñπÂºè|ËÅîÁ≥ªÁîµËØù)[:Ôºö]?\s*([0-9\-\+\(\)\s]{6,20})/i,
        /([0-9\-\+\(\)\s]{8,15})/ // Áõ¥Êé•ÂåπÈÖçÊâãÊú∫Âè∑
      ]
      for (const pattern of phonePatterns) {
        const match = text.match(pattern)
        if (match && match[2]) {
          phone = match[2].trim()
          console.log('Found phone:', phone) // Ë∞ÉËØïÊó•Âøó
          break
        } else if (match && match[1] && !phone) {
          phone = match[1].trim()
          console.log('Found phone (direct):', phone) // Ë∞ÉËØïÊó•Âøó
        }
      }

      console.log('Updating orderInfo:', { items, name, phone }) // Ë∞ÉËØïÊó•Âøó
      setOrderInfo({ items, name, phone })
    }
  }

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      disconnect()
    }
  }, [])

  // Auto-connect on mount
  useEffect(() => {
    const attemptAutoConnect = async () => {
      console.log('Auto-connecting to Realtime API via WebRTC')
      await connect()
    }
    attemptAutoConnect()
  }, [])

  // New Voice WS demo (optional) when user toggles
  useEffect(() => {
    if (!useNewVoiceWS) return
    voiceHookRef.current = useVoiceChat()
    voiceHookRef.current.connect()
    return () => {
      // no explicit disconnect API in hook; WS will close on page unload
    }
  }, [useNewVoiceWS])

  return (
    <div className="App">
      <h1>üéôÔ∏è AI Voice Agent Tester</h1>
      <div style={{margin: '1rem 0'}}>
        <label htmlFor="model-select" style={{marginRight: '10px', fontWeight: 'bold'}}>Model:</label>
        <select 
          id="model-select"
          value={selectedModel} 
          onChange={(e) => setSelectedModel(e.target.value)}
          disabled={isConnected}
          style={{padding: '8px 12px', fontSize: '14px', borderRadius: '4px', border: '1px solid #ccc'}}
        >
          <option value="openai">OpenAI GPT-4o Realtime</option>
          <option value="grok">Grok Voice Agent</option>
          <option value="local">üè† Local Model (Fun-ASR + CosyVoice)</option>
        </select>
        {isConnected && <span style={{marginLeft: '10px', color: '#666', fontSize: '14px'}}>‚ö†Ô∏è Disconnect to change model</span>}
      </div>
      <p>Connect to {selectedModel === 'openai' ? 'ChatGPT Realtime API via WebRTC' : selectedModel === 'grok' ? 'Grok Voice Agent API via WebSocket' : 'Local Voice Service (ASR + LLM + TTS)'}</p>

      <div className="voice-panel">
        {error && (
          <div className="error-message">
            {error}
          </div>
        )}
        
        {!isConnected && (
          <div style={{ textAlign: 'center', padding: '20px' }}>
            <p>Connecting to API...</p>
            <button onClick={connect} style={{marginTop:'8px'}}>Connect</button>
          </div>
        )}

        {isConnected && (
          <>
            <div className={`status ${connectionStatus}`}>
              Status: {connectionStatus === 'connected' ? 'üü¢ Connected' : 'üî¥ Disconnected'}
            </div>

            <div className="controls">
              <div>üé§ Microphone streaming is active via WebRTC.</div>
            </div>

            <div className="button-group">
              <button onClick={disconnect}>Disconnect</button>
            </div>

            {selectedModel === 'local' && (
              <div style={{marginTop:'1rem',padding:'1rem',border:'1px dashed #ccc',borderRadius:'8px'}}>
                <div style={{display:'flex',alignItems:'center',gap:'8px'}}>
                  <input id="toggle-new-ws" type="checkbox" checked={useNewVoiceWS} onChange={(e)=>setUseNewVoiceWS(e.target.checked)} />
                  <label htmlFor="toggle-new-ws">(ÂèØÈÄâ) `/ws/voice` WebSocketÔºàÈªòËÆ§‰ΩøÁî® REST /api/voice/streamÔºâ</label>
                </div>
                {useNewVoiceWS ? (
                  <div style={{marginTop:'0.5rem',display:'flex',gap:'8px',flexWrap:'wrap'}}>
                    <button onClick={()=>{voiceHookRef.current?.startRecording(); setMicError('')}}>Start Mic</button>
                    <button onClick={()=>voiceHookRef.current?.stopRecording()}>Stop Mic</button>
                    <button onClick={()=>voiceHookRef.current?.sendText('‰Ω†Â•ΩÔºåÂ∏ÆÊàëÁÇπ‰∏Ä‰ªΩÂÆ´‰øùÈ∏°‰∏ÅÂíåÁ±≥È•≠')}>Send Sample Text</button>
                    <button onClick={()=>voiceHookRef.current?.clear()}>Clear Conversation</button>
                    <button onClick={()=>{try {voiceHookRef.current?.startRecording(); setMicError('')} catch(e){setMicError('È∫¶ÂÖãÈ£éÊú™ÂºÄÂêØÔºåËØ∑ÂÖÅËÆ∏ÊùÉÈôêÂêéÈáçËØï');}}}>Retry Mic</button>
                    <button onClick={()=>{connectLocal();}}>Retry Connect</button>
                    <button onClick={runUnifiedStream}>Record 2s & Stream (REST)</button>
                  </div>
                ) : (
                  <div style={{marginTop:'0.5rem',display:'flex',gap:'8px',flexWrap:'wrap'}}>
                    <button onClick={runUnifiedStream}>ÂΩï 2 ÁßíÂπ∂Ë∞ÉÁî® /api/voice/stream</button>
                    <button onClick={()=>{appendStreamLog('ÊâãÂä®Âà∑Êñ∞ÊµÅ'); setStreamStatus('idle')}}>Ê∏ÖÁ©∫Êó•Âøó</button>
                  </div>
                )}
                {micError && (
                  <div style={{marginTop:'0.5rem',color:'#d9534f'}}>{micError}</div>
                )}
                {streamStatus !== 'idle' && (
                  <div style={{marginTop:'0.75rem'}}>
                    <div style={{fontWeight:'bold'}}>Unified Stream Log ({streamStatus})</div>
                    <div style={{maxHeight:'160px',overflowY:'auto',border:'1px solid #eee',padding:'8px',borderRadius:'6px',background:'#fafafa'}}>
                      {streamLog.length === 0 && <div style={{color:'#999'}}>Waiting for frames...</div>}
                      {streamLog.map((item, idx) => (
                        <div key={idx} style={{fontSize:'12px'}}>
                          <span style={{opacity:0.6, marginRight:'6px'}}>{item.ts}</span>{item.msg}
                        </div>
                      ))}
                    </div>
                  </div>
                )}
              </div>
            )}

            <div className="order-info" style={{margin:'1rem 0',padding:'1rem',border:'1px solid #eee',borderRadius:'8px'}}>
              <h4>Â∑≤Êî∂ÈõÜÁöÑÁÇπÂçï‰ø°ÊÅØ</h4>
              <div><strong>ËèúÂìÅÔºö</strong>{orderInfo.items || <span style={{color:'#888'}}>Êú™Â°´ÂÜô</span>}</div>
              <div><strong>ÂßìÂêçÔºö</strong>{orderInfo.name || <span style={{color:'#888'}}>Êú™Â°´ÂÜô</span>}</div>
              <div><strong>ÁîµËØùÔºö</strong>{orderInfo.phone || <span style={{color:'#888'}}>Êú™Â°´ÂÜô</span>}</div>
            </div>

            <div className="transcript">
              <h4>Conversation</h4>
              {transcript.length === 0 && (
                <p style={{ color: '#888', textAlign: 'center' }}>
                  Start talking...
                </p>
              )}
              {transcript.map((item, index) => (
                <div
                  key={index}
                  className={`transcript-item ${item.role}`}
                  style={{ opacity: item.isPartial ? 0.7 : 1 }}
                >
                  <strong>{item.role === 'user' ? 'You' : 'AI'}:</strong> {item.text}
                  {item.isPartial && <span style={{ marginLeft: '5px', opacity: 0.5 }}>...</span>}
                  <span style={{ fontSize: '0.8em', marginLeft: '10px', opacity: 0.6 }}>
                    {item.timestamp}
                  </span>
                </div>
              ))}
            </div>
          </>
        )}
      </div>

      <div style={{ marginTop: '2rem', fontSize: '0.9em', opacity: 0.7 }}>
        <p>
          Using WebRTC connection with ephemeral keys for secure browser-based voice interaction.
          <br />
          Server must be running on port 3000 to mint ephemeral tokens.
        </p>
      </div>
    </div>
  )
}

export default App

#!/usr/bin/env python3
"""
LLM æœåŠ¡ - å¯¹è¯å¼•æ“
æ”¯æŒè¿œç¨‹ API (OpenAI/Anthropic) å’Œæœ¬åœ°æ¨¡å‹ (Ollama/Qwen)
"""

import os
from typing import AsyncGenerator, List, Dict
from loguru import logger

try:
    from openai import AsyncOpenAI
except ImportError:
    logger.warning("OpenAI SDK not installed")
    AsyncOpenAI = None


class LLMService:
    """LLM å¯¹è¯æœåŠ¡"""
    
    def __init__(self):
        self.use_local = os.getenv("USE_LOCAL_LLM", "1") == "1"  # é»˜è®¤ä½¿ç”¨æœ¬åœ°
        self.api_key = os.getenv("LLM_API_KEY", "")
        
        # æœ¬åœ° Ollama é…ç½®
        self.ollama_base = os.getenv("OLLAMA_BASE_URL", "http://host.docker.internal:11434/v1")
        self.ollama_model = os.getenv("OLLAMA_MODEL", "qwen2.5:0.5b")
        
        # è¿œç¨‹ API é…ç½®
        self.api_base = os.getenv("LLM_API_BASE", "https://api.openai.com/v1")
        self.model = os.getenv("LLM_MODEL", "gpt-4o-mini")
        
        # ç³»ç»Ÿæç¤ºè¯ (ä»ç°æœ‰é…ç½®å¤åˆ¶)
        self.system_prompt = """
You are a voice order-taking AI agent for Chunky Chook (Chicken & Chips) in Auckland.
Your job is to take accurate pickup orders quickly, confirm details, and avoid mistakes.

## Restaurant Information
- Name: Chunky Chook (Chicken & Chips)
- Location: Auckland, New Zealand
- Specialty: Fried chicken and chips
- Operating Hours: 11:00 AM - 9:00 PM

## Your Responsibilities
1. Greet customers warmly
2. Take their order accurately
3. Confirm items, quantities, and special requests
4. Get customer name and phone number for pickup
5. Provide estimated pickup time (typically 15-20 minutes)
6. Thank them and confirm the order

## Guidelines
- Be friendly but efficient
- Clarify any unclear requests
- Suggest popular items if asked
- Always confirm the complete order before finishing
- Speak naturally and conversationally
""".strip()
        
        self.client = None
        
    async def initialize(self):
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        if AsyncOpenAI is None:
            raise RuntimeError("OpenAI SDK not installed. Run: pip install openai")
        
        if self.use_local:
            # ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹
            logger.info(f"Initializing local LLM: {self.ollama_model}")
            logger.info(f"Ollama endpoint: {self.ollama_base}")
            
            self.client = AsyncOpenAI(
                api_key="ollama",  # Ollama ä¸éœ€è¦çœŸå® API key
                base_url=self.ollama_base
            )
            self.model = self.ollama_model
            
            # æµ‹è¯•è¿æ¥
            try:
                # ç®€å•æµ‹è¯•è¯·æ±‚
                logger.info("Testing Ollama connection...")
                await self._test_connection()
                logger.success(f"âœ… Local LLM initialized: {self.ollama_model}")
            except Exception as e:
                logger.error(f"Failed to connect to Ollama: {e}")
                logger.warning("Ollama å¯èƒ½æœªè¿è¡Œï¼Œè¯·ç¡®ä¿:")
                logger.warning("  1. Mac æœ¬åœ°: ollama serve")
                logger.warning("  2. Docker: ä½¿ç”¨ host.docker.internal")
                raise
        else:
            # ä½¿ç”¨è¿œç¨‹ API
            self.client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.api_base
            )
            logger.success(f"âœ… LLM service initialized (API: {self.api_base})")
    
    async def _test_connection(self):
        """æµ‹è¯• Ollama è¿æ¥"""
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "Hi"}],
                max_tokens=5,
                stream=False
            )
            logger.info(f"Connection test successful: {response.choices[0].message.content[:20]}")
        except Exception as e:
            raise ConnectionError(f"Cannot connect to Ollama: {e}")
    
    async def chat_stream(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 500
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼å¯¹è¯
        
        Args:
            messages: å¯¹è¯å†å² [{"role": "user", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        Yields:
            æ–‡æœ¬å¢é‡
        """
        if self.client is None:
            await self.initialize()
        
        try:
            # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
            full_messages = [
                {"role": "system", "content": self.system_prompt}
            ] + messages
            
            # æµå¼è¯·æ±‚
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=full_messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True
            )
            
            # è¿”å›æ–‡æœ¬æµ
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"LLM chat error: {e}")
            yield f"[Error: {str(e)}]"
    
    async def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7
    ) -> str:
        """éæµå¼å¯¹è¯"""
        if self.client is None:
            await self.initialize()
        
        try:
            full_messages = [
                {"role": "system", "content": self.system_prompt}
            ] + messages
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=full_messages,
                temperature=temperature
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"LLM chat error: {e}")
            return f"æŠ±æ­‰ï¼Œå‡ºç°é”™è¯¯: {str(e)}"


# ============================================
# ğŸ’¬ ä½¿ç”¨ç¤ºä¾‹
# ============================================
"""
# åˆå§‹åŒ–
llm = LLMService()

# æµå¼å¯¹è¯
messages = [
    {"role": "user", "content": "æˆ‘æƒ³ç‚¹ä¸€ä»½ç‚¸é¸¡"}
]

async for token in llm.chat_stream(messages):
    print(token, end="", flush=True)

# éæµå¼
response = await llm.chat(messages)
print(response)
"""
